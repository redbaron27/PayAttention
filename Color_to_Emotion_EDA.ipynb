{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Color_to_Emotion_EDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6WGD5p-KB6vZ",
        "colab": {}
      },
      "source": [
        "import ipywidgets as widgets\n",
        "from ipywidgets import HBox, VBox\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import pandas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_JUzLSDHClwP",
        "colab": {}
      },
      "source": [
        "import sys \n",
        "from termcolor import colored, cprint \n",
        "\n",
        "def fullShadeWidget(shadedPics, emotion1, emotion2, emotion3):\n",
        "    \"\"\"Iterates through the photos in shadedPics and displays them with\n",
        "    an interactive widget. A legend for the different emotions is also\n",
        "    produced.\n",
        "    @shadedPics: path to folder of images to be displayed\n",
        "    @emotion1: first emotion name\n",
        "    @emotion2: second emotion name\n",
        "    @emotion3: third emotion name\n",
        "    \"\"\"\n",
        "    #following three functions taken from:\n",
        "    #https://www.geeksforgeeks.org/print-colors-python-terminal/\n",
        "    def prRed(skk): \n",
        "        print(\"\\033[91m {}\\033[00m\" .format(skk))\n",
        "    def prGreen(skk): \n",
        "        print(\"\\033[92m {}\\033[00m\" .format(skk))\n",
        "    def prBlue(skk): \n",
        "        print(\"\\033[96m {}\\033[00m\" .format(skk)) \n",
        "        \n",
        "    \n",
        "    img_lst = []\n",
        "    for i in sorted(os.listdir(shadedPics)):\n",
        "      img_lst += [i]\n",
        "\n",
        "    @widgets.interact\n",
        "    def f(x=(0, len(img_lst) - 1)):\n",
        "        name = img_lst[x]\n",
        "        print(name)\n",
        "        print()\n",
        "        prRed(\"RED\")\n",
        "        print(\" = \" + emotion1)\n",
        "        print()\n",
        "        prGreen(\"GREEN\")\n",
        "        print(\" = \" + emotion2)\n",
        "        print()\n",
        "        prBlue(\"BLUE\")\n",
        "        print(\" = \" + emotion3)\n",
        "        print()\n",
        "        img = mpimg.imread(shadedPics + \"/\" + name)\n",
        "        plt.imshow(img)\n",
        "        # USE MATPLOT to change make bigger all the stuff you know...\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6lQ6jLNMGq_2",
        "colab": {}
      },
      "source": [
        "\n",
        "class RGBTransform(object):\n",
        "    #code taken from William Chargin\n",
        "    #https://gist.github.com/WChargin/d8eb0cbafc4d4479d004\n",
        "    #article he wrote about code:\n",
        "    #http://engineering.khanacademy.org/posts/making-thumbnails-fast.htm\n",
        "\n",
        "    #this code allows us to shade an image\n",
        "    \"\"\"A description of an affine transformation to an RGB image.\n",
        "    This class is immutable.\n",
        "    Methods correspond to matrix left-multiplication/post-application:\n",
        "    for example,\n",
        "        RGBTransform().multiply_with(some_color).desaturate()\n",
        "    describes a transformation where the multiplication takes place first.\n",
        "    Use rgbt.applied_to(image) to return a converted copy of the given image.\n",
        "    For example:\n",
        "        grayish = RGBTransform.desaturate(factor=0.5).applied_to(some_image)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, matrix=None):\n",
        "        self._matrix = matrix if matrix is not None else np.eye(4)\n",
        "\n",
        "    def _then(self, operation):\n",
        "        return RGBTransform(np.dot(_embed44(operation), self._matrix))\n",
        "\n",
        "    def desaturate(self, factor=1.0, weights=(0.299, 0.587, 0.114)):\n",
        "        \"\"\"Desaturate an image by the given amount.\n",
        "        A factor of 1.0 will make the image completely gray;\n",
        "        a factor of 0.0 will leave the image unchanged.\n",
        "        The weights represent the relative contributions of each channel.\n",
        "        They should be a 1-by-3 array-like object (tuple, list, np.array).\n",
        "        In most cases, their values should sum to 1.0\n",
        "        (otherwise, the transformation will cause the image\n",
        "        to get lighter or darker).\n",
        "        \"\"\"\n",
        "        weights = _to_rgb(weights, \"weights\")\n",
        "\n",
        "        # tile: [wr, wg, wb]  ==>  [[wr, wg, wb], [wr, wg, wb], [wr, wg, wb]]\n",
        "        desaturated_component = factor * np.tile(weights, (3, 1))\n",
        "        saturated_component = (1 - factor) * np.eye(3)\n",
        "        operation = desaturated_component + saturated_component\n",
        "\n",
        "        return self._then(operation)\n",
        "\n",
        "    def multiply_with(self, base_color, factor=1.0):\n",
        "        \"\"\"Multiply an image by a constant base color.\n",
        "        The base color should be a 1-by-3 array-like object\n",
        "        representing an RGB color in [0, 255]^3 space.\n",
        "        For example, to multiply with orange,\n",
        "        the transformation\n",
        "            RGBTransform().multiply_with((255, 127, 0))\n",
        "        might be used.\n",
        "        The factor controls the strength of the multiplication.\n",
        "        A factor of 1.0 represents straight multiplication;\n",
        "        other values will be linearly interpolated between\n",
        "        the identity (0.0) and the straight multiplication (1.0).\n",
        "        \"\"\"\n",
        "        component_vector = _to_rgb(base_color, \"base_color\") / 255.0\n",
        "        new_component = factor * np.diag(component_vector)\n",
        "        old_component = (1 - factor) * np.eye(3)\n",
        "        operation = new_component + old_component\n",
        "\n",
        "        return self._then(operation)\n",
        "\n",
        "    def mix_with(self, base_color, factor=1.0):\n",
        "        \"\"\"Mix an image by a constant base color.\n",
        "        The base color should be a 1-by-3 array-like object\n",
        "        representing an RGB color in [0, 255]^3 space.\n",
        "        For example, to mix with orange,\n",
        "        the transformation\n",
        "            RGBTransform().mix_with((255, 127, 0))\n",
        "        might be used.\n",
        "        The factor controls the strength of the color to be added.\n",
        "        If the factor is 1.0, all pixels will be exactly the new color;\n",
        "        if it is 0.0, the pixels will be unchanged.\n",
        "        \"\"\"\n",
        "        base_color = _to_rgb(base_color, \"base_color\")\n",
        "        operation = _embed44((1 - factor) * np.eye(3))\n",
        "        operation[:3, 3] = factor * base_color\n",
        "\n",
        "        return self._then(operation)\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"Get the underlying 3-by-4 matrix for this affine transform.\"\"\"\n",
        "        return self._matrix[:3, :]\n",
        "\n",
        "    def applied_to(self, image):\n",
        "        \"\"\"Apply this transformation to a copy of the given RGB* image.\n",
        "        The image should be a PIL image with at least three channels.\n",
        "        Specifically, the RGB and RGBA modes are both supported, but L is not.\n",
        "        Any channels past the first three will pass through unchanged.\n",
        "        The original image will not be modified;\n",
        "        a new image of the same mode and dimensions will be returned.\n",
        "        \"\"\"\n",
        "\n",
        "        # PIL.Image.convert wants the matrix as a flattened 12-tuple.\n",
        "        # (The docs claim that they want a 16-tuple, but this is wrong;\n",
        "        # cf. _imaging.c:767 in the PIL 1.1.7 source.)\n",
        "        matrix = tuple(self.get_matrix().flatten())\n",
        "\n",
        "        channel_names = image.getbands()\n",
        "        channel_count = len(channel_names)\n",
        "        if channel_count < 3:\n",
        "            raise ValueError(\"Image must have at least three channels!\")\n",
        "        elif channel_count == 3:\n",
        "            return image.convert('RGB', matrix)\n",
        "        else:\n",
        "            # Probably an RGBA image.\n",
        "            # Operate on the first three channels (assuming RGB),\n",
        "            # and tack any others back on at the end.\n",
        "            channels = list(image.split())\n",
        "            rgb = PIL.Image.merge('RGB', channels[:3])\n",
        "            transformed = rgb.convert('RGB', matrix)\n",
        "            new_channels = transformed.split()\n",
        "            channels[:3] = new_channels\n",
        "            return PIL.Image.merge(''.join(channel_names), channels)\n",
        "\n",
        "    def applied_to_pixel(self, color):\n",
        "        \"\"\"Apply this transformation to a single RGB* pixel.\n",
        "        In general, you want to apply a transformation to an entire image.\n",
        "        But in the special case where you know that the image is all one color,\n",
        "        you can save cycles by just applying the transformation to that color\n",
        "        and then constructing an image of the desired size.\n",
        "        For example, in the result of the following code,\n",
        "        image1 and image2 should be identical:\n",
        "            rgbt = create_some_rgb_tranform()\n",
        "            white = (255, 255, 255)\n",
        "            size = (100, 100)\n",
        "            image1 = rgbt.applied_to(PIL.Image.new(\"RGB\", size, white))\n",
        "            image2 = PIL.Image.new(\"RGB\", size, rgbt.applied_to_pixel(white))\n",
        "        The construction of image2 will be faster for two reasons:\n",
        "        first, only one PIL image is created; and\n",
        "        second, the transformation is only applied once.\n",
        "        The input must have at least three channels;\n",
        "        the first three channels will be interpreted as RGB,\n",
        "        and any other channels will pass through unchanged.\n",
        "        To match the behavior of PIL,\n",
        "        the values of the resulting pixel will be rounded (not truncated!)\n",
        "        to the nearest whole number.\n",
        "        \"\"\"\n",
        "        color = tuple(color)\n",
        "        channel_count = len(color)\n",
        "        extra_channels = tuple()\n",
        "        if channel_count < 3:\n",
        "            raise ValueError(\"Pixel must have at least three channels!\")\n",
        "        elif channel_count > 3:\n",
        "            color, extra_channels = color[:3], color[3:]\n",
        "\n",
        "        color_vector = np.array(color + (1, )).reshape(4, 1)\n",
        "        result_vector = np.dot(self._matrix, color_vector)\n",
        "        result = result_vector.flatten()[:3]\n",
        "\n",
        "        full_result = tuple(result) + extra_channels\n",
        "        rounded = tuple(int(round(x)) for x in full_result)\n",
        "\n",
        "        return rounded\n",
        "\n",
        "\n",
        "def _embed44(matrix):\n",
        "    \"\"\"Embed a 4-by-4 or smaller matrix in the upper-left of I_4.\"\"\"\n",
        "    result = np.eye(4)\n",
        "    r, c = matrix.shape\n",
        "    result[:r, :c] = matrix\n",
        "    return result\n",
        "\n",
        "\n",
        "def _to_rgb(thing, name=\"input\"):\n",
        "    \"\"\"Convert an array-like object to a 1-by-3 numpy array, or fail.\"\"\"\n",
        "    thing = np.array(thing)\n",
        "    assert thing.shape == (3, ), (\n",
        "        \"Expected %r to be a length-3 array-like object, but found shape %s\" %\n",
        "            (name, thing.shape))\n",
        "    return thing\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87sp6EnOxHLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shadeFace(faceLocations, imagePath, output, firstVal, secondVal, thirdVal, name):\n",
        "    \"\"\"Shades the faces in one image.\n",
        "    @facelocations: a 2D array representing the locations of all the faces in an image. Each\n",
        "    inner array contains 4 ints representing the pixels of the face's 4 corners. The 4 corners\n",
        "    must be left, upper, right, lower.\n",
        "    @imagePath: a path to the image to be shaded\n",
        "    @output: a path to the output folder where the new image \n",
        "    @firstVal: an array of the proportions of the first emotion\n",
        "    @secondVal: an array of the proportions of the second emotion\n",
        "    @thirdVal: an array of the proportions of the third emotion\n",
        "    @name: the name for the new shaded image\n",
        "    \"\"\"\n",
        "    #read image\n",
        "    img = Image.open(imagePath)\n",
        "    \n",
        "    #store the cropped faces into a list\n",
        "    imgFaces = []\n",
        "    for i in faceLocations:\n",
        "        if (len(i) != 0):\n",
        "            imgFaces.append(img.crop((i[0], i[1], i[2], i[3])))\n",
        "        else:\n",
        "            imgFaces.append(0)\n",
        "    \n",
        "    #shade each cropped face accordng to firstVal, secondVal, and thirdVal and store in newFaces\n",
        "    newFaces = []\n",
        "    count = 0\n",
        "    for i in imgFaces:\n",
        "        if (i != 0):\n",
        "            shadedFace = i.convert('RGB')\n",
        "            redMix = firstVal[count] * 255\n",
        "            greenMix = secondVal[count] * 255\n",
        "            blueMix = thirdVal[count] * 255\n",
        "            shadedFace = RGBTransform().mix_with((redMix, greenMix, blueMix),factor=.6).applied_to(shadedFace)\n",
        "            newFaces.append(shadedFace)\n",
        "            count += 1\n",
        "        else:\n",
        "            count += 1\n",
        "        \n",
        "    #paste each new shaded face onto a copy of the original image\n",
        "    count = 0\n",
        "    img = img.convert('RGB')\n",
        "    for i in faceLocations:\n",
        "        #Image.Image.paste(img, newFaces[count], (i[0], i[1], i[2], i[3]) )\n",
        "        if (len(i) != 0):\n",
        "            img.paste(newFaces[count], (i[0], i[1], i[2], i[3]))\n",
        "            count += 1\n",
        "    \n",
        "    \n",
        "    #sets working directory to 'output'\n",
        "    os.chdir(output)\n",
        "    #saves pic with shaded faces into 'output'\n",
        "    img.save(name + \".jpg\", \"JPEG\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tzz1ZZWYxHLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aggregateShadeFace(imagePath, DFPath, output, faceLocations):\n",
        "    \"\"\"Iterates through each photo in an folder and shades its individual faces according\n",
        "    to the emotions of each person.\n",
        "    @imagePath: path to the folder with the unshaded images\n",
        "    @DFPath: path to the folder with each individual image's dataframe. Dataframes must have a\n",
        "    column of the proportions of emotions for each face in the image and nothing else. Dataframes\n",
        "    must be in the same order in the folder as the images in imagePath.\n",
        "    @output: path to folder where shaded images will be saved\n",
        "    @faceLocations: a 3D array representing all the facelocations for all the faces for all\n",
        "    the images. 1st dimension should be the image, 2nd dimension should be the face, 3d dimension should\n",
        "    be the face corners. The corners are pixel ints and must be saved in ascending order.  \n",
        "    \"\"\"\n",
        "    #set the working directory to DFPath and store dataframe names to DFNames\n",
        "    os.chdir(DFPath)\n",
        "    DFNames = os.listdir()\n",
        "    #set the working directory to imagePath and store image names to imageNames\n",
        "    os.chdir(imagePath)\n",
        "    imageNames = os.listdir()\n",
        "    #determine if using 2 or 3 emotions\n",
        "    testDFPath = DFPath + \"/\" + DFNames[0]\n",
        "    testDF = pandas.read_csv(testDFPath)\n",
        "    hasThreeEmotions = True\n",
        "    if (len(testDF.columns) != 3):\n",
        "        hasThreeEmotions = False\n",
        "    \n",
        "    #iteratire through each image. Call shadeFace for that image's faceLocations and emotions\n",
        "    for i in np.arange(len(imageNames)):\n",
        "        if (hasThreeEmotions):\n",
        "            theDF = DFPath + \"/\" + DFNames[i]\n",
        "            theDF = pandas.read_csv(theDF)\n",
        "            theFile = imagePath + \"/\" + imageNames[i]\n",
        "            firstVal = theDF.iloc[:,0]\n",
        "            secondVal = theDF.iloc[:,1]\n",
        "            thirdVal = theDF.iloc[:,2]\n",
        "            firstVal = pandas.Series.to_numpy(firstVal)\n",
        "            secondVal = pandas.Series.to_numpy(secondVal)\n",
        "            thirdVal = pandas.Series.to_numpy(thirdVal)\n",
        "            shadeFace(faceLocations[i], theFile, output, firstVal, secondVal, thirdVal, \"faceshaded_\" + imageNames[i])\n",
        "        if (not hasThreeEmotions):\n",
        "            theDF = DFPath + \"/\" + DFNames[i]\n",
        "            theDF = pandas.read_csv(theDF)\n",
        "            theFile = imagePath + \"/\" + imageNames[i]\n",
        "            firstVal = theDF.iloc[:,0]\n",
        "            secondVal = theDF.iloc[:,1]\n",
        "            firstVal = pandas.Series.to_numpy(firstVal)\n",
        "            secondVal = pandas.Series.to_numpy(secondVal)\n",
        "            thirdVal = np.zeros(len(firstVal))\n",
        "            shadeFace(faceLocations[i], theFile, output, firstVal, secondVal, thirdVal, \"faceshaded_\" + imageNames[i])\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV9cP7SkxHL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imageShading (imagePath, output, firstVal, secondVal, thirdVal, name):\n",
        "    '''Shades an entire image from ImagePath based on values. Saves as name to output.\n",
        "    @image_path: path to image that will be shaded\n",
        "    @output: where shaded image will be stored\n",
        "    @firstVal: decimal representing strength of first emotion\n",
        "    @secondVal: decimal representing strength of second emotion\n",
        "    @thirdVal: decimal representing strength of third emotion\n",
        "    @name: what the shaded image will be named '''\n",
        "    #code does not work if more than 3 emotions; does work with 2 emotions\n",
        "    #if you have only 3 emotions, set thirdVal to 0\n",
        "    \n",
        "    \n",
        "    #inspired by this post:\n",
        "    #https://stackoverflow.com/questions/32578346/how-to-change-color-of-image-using-python\n",
        "    #This code uses a python library (transofmrs.py) developed by William Chargin\n",
        "    #source: https://gist.github.com/WChargin/d8eb0cbafc4d4479d004\n",
        "    #Chargin's article about the code:\n",
        "    #http://engineering.khanacademy.org/posts/making-thumbnails-fast.htm\n",
        "    \n",
        "    \n",
        "    #firstVal is red, secondVal is green, thirdVal is blue\n",
        "    \n",
        "    pic = Image.open(imagePath)\n",
        "    pic = pic.convert('RGB')\n",
        "    \n",
        "    #determine what proportion of RGB the image should be\n",
        "    redMix = firstVal * 255\n",
        "    greenMix = secondVal * 255\n",
        "    blueMix = thirdVal * 255\n",
        "    \n",
        "    #shade the pic according to proportions, with a strength of 0.3\n",
        "    pic = RGBTransform().mix_with((redMix, greenMix, blueMix),factor=.75).applied_to(pic)\n",
        "    \n",
        "    #sets working directory to 'output'\n",
        "    os.chdir(output)\n",
        "    #saves pic into 'output'\n",
        "    pic.save(name + \".jpg\", \"JPEG\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh187UkjxHL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aggregateFullShade(df, input, output):\n",
        "    \"\"\"Iterates through all the photos in a folder and shades the entire image according to their emotions.\n",
        "    @df: dataframe containing the proportion of emotions for each image. Can have 2 or 3 emotions. Dataframe\n",
        "    must have the columns in the order \"picture names, emotions, dominant emotion\"\n",
        "    @input: folder with images to be shaded\n",
        "    @output: folder where shaded images will be saved\n",
        "    \"\"\"\n",
        "    #check to see new dataframe format\n",
        "    #chop off first and last column of df\n",
        "    df.iloc[:,-1]\n",
        "    newDF = df\n",
        "    newDF.drop([0])\n",
        "    #list of emotion names\n",
        "    emotions = newDF.columns\n",
        "    #set working directory to input and get list of file names\n",
        "    os.chdir(input)\n",
        "    photoNames = os.listdir()\n",
        "    #iterate through and shade each file\n",
        "    for i in np.arange(len(photoNames)):\n",
        "        if (len(emotions) == 3):\n",
        "            theFile = input + \"/\" + photoNames[i]\n",
        "            imageShading(theFile, output, newDF.loc[i][1], newDF.loc[i][2], newDF.loc[i][3], \"shaded_\" + photoNames[i])\n",
        "        else:\n",
        "            theFile = input + \"/\" + photoNames[i]\n",
        "            imageShading(theFile, output, newDF.loc[i][1], newDF.loc[i][2], 0, \"shaded_\" + photoNames[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5OhHvz2xHL7",
        "colab_type": "code",
        "colab": {
          "referenced_widgets": [
            "2efa9e25613643f69f03f4ea9c4534ed"
          ]
        },
        "outputId": "cbdbb58e-70aa-444b-d030-5163beb1218f"
      },
      "source": [
        "#example of how functions can be called to create a widget displaying the different shaded faces\n",
        "\n",
        "theFaceLocations = [[[253, 425, 408, 580], [811, 440, 1034, 663], [306, 175, 414, 283]],\n",
        " [[811, 440, 1034, 663], [242, 407, 428, 593], [318, 187, 426, 295]],\n",
        " [[253, 425, 408, 580], [811, 440, 1034, 663], [311, 168, 440, 297]],\n",
        " [[811, 440, 1034, 663], [270, 425, 425, 580], [306, 163, 414, 271]],\n",
        " [[270, 425, 425, 580], [795, 409, 1062, 676]],\n",
        " [[811, 415, 1034, 638], [270, 425, 425, 580]],\n",
        " [[835, 415, 1058, 638]],\n",
        " [[885, 489, 1108, 712]],\n",
        " [[885, 464, 1108, 687]],\n",
        " [[210, 383, 339, 512], [854, 468, 1122, 720]],\n",
        " [[825, 468, 1092, 720], [882, 57, 1067, 242], [159, 407, 345, 593]],\n",
        " [[825, 52, 1092, 320], [180, 387, 366, 573], [812, 420, 1133, 720]],\n",
        " [[825, 439, 1092, 706], [201, 391, 356, 546]],\n",
        " [[201, 366, 386, 552], [825, 439, 1092, 706], [835, 68, 1058, 291]],\n",
        " [[825, 379, 1092, 647], [218, 391, 373, 546]]]\n",
        "\n",
        "#example paths\n",
        "#contains paths to original, unshaded images\n",
        "newInput = r\"C:\\Users\\Jason\\Matt School Stuff\\finalInput\"\n",
        "#where the images will be saved once shaded\n",
        "newOutput = r\"C:\\Users\\Jason\\Matt School Stuff\\finalTest\"\n",
        "#path to DFs for each image's faces\n",
        "newDFPath = r\"C:\\Users\\Jason\\Matt School Stuff\\finalDFs\"\n",
        "\n",
        "aggregateShadeFace(newInput, newDFPath, newOutput, theFaceLocations)\n",
        "\n",
        "fullShadeWidget(newOutput, \"disgust\", \"surprise\", \"happiness\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2efa9e25613643f69f03f4ea9c4534ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(IntSlider(value=7, description='x', max=14), Output()), _dom_classes=('widget-interact',â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttQ-iqBUxHL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}